{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handles PDF, PPTX and Txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pptx import Presentation\n",
    "\n",
    "class DocumentReader:\n",
    "    \"\"\"A class for reading and extracting text from various document formats.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Initialize the DocumentReader with a file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the document file\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the specified file does not exist\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "            \n",
    "        self.file_path = file_path\n",
    "        self.file_extension = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    def extract_text(self):\n",
    "        \"\"\"\n",
    "        Extract text content from the document based on its file type.\n",
    "\n",
    "        Returns:\n",
    "            str: Extracted text content from the document\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the file type is not supported\n",
    "        \"\"\"\n",
    "        handlers = {\n",
    "            '.txt': self._extract_text_from_txt,\n",
    "            '.pdf': self._extract_text_from_pdf,\n",
    "            '.pptx': self._extract_text_from_pptx\n",
    "        }\n",
    "\n",
    "        handler = handlers.get(self.file_extension)\n",
    "        if handler:\n",
    "            return handler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {self.file_extension}\")\n",
    "\n",
    "    def _extract_text_from_txt(self):\n",
    "        \"\"\"\n",
    "        Extract text from a plain text file.\n",
    "\n",
    "        Returns:\n",
    "            str: Content of the text file\n",
    "        \"\"\"\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def _extract_text_from_pdf(self):\n",
    "        \"\"\"\n",
    "        Extract text from a PDF file, including text from images using OCR.\n",
    "\n",
    "        Returns:\n",
    "            str: Combined text content from all PDF pages\n",
    "        \"\"\"\n",
    "        doc = fitz.open(self.file_path)\n",
    "        text_contents = []\n",
    "\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text = page.get_text(\"text\")\n",
    "\n",
    "            if text.strip():\n",
    "                text_contents.append(text)\n",
    "            else:\n",
    "                # If no text is found, try OCR\n",
    "                text_contents.append(self._extract_text_from_pdf_image(page))\n",
    "\n",
    "        return '\\n'.join(text_contents)\n",
    "\n",
    "    def _extract_text_from_pdf_image(self, page):\n",
    "        \"\"\"\n",
    "        Extract text from an image-based PDF page using OCR.\n",
    "\n",
    "        Args:\n",
    "            page: PyMuPDF Page object\n",
    "\n",
    "        Returns:\n",
    "            str: Extracted text from the page image\n",
    "        \"\"\"\n",
    "        zoom = 2.0  # Higher zoom factor for better OCR accuracy\n",
    "        matrix = fitz.Matrix(zoom, zoom)\n",
    "        pixmap = page.get_pixmap(matrix=matrix)\n",
    "        \n",
    "        image = Image.frombytes(\n",
    "            \"RGB\", \n",
    "            [pixmap.width, pixmap.height], \n",
    "            pixmap.samples\n",
    "        )\n",
    "        \n",
    "        return pytesseract.image_to_string(image)\n",
    "\n",
    "    def _extract_text_from_pptx(self):\n",
    "        \"\"\"\n",
    "        Extract text from a PowerPoint presentation.\n",
    "\n",
    "        Returns:\n",
    "            str: Combined text content from all slides\n",
    "        \"\"\"\n",
    "        presentation = Presentation(self.file_path)\n",
    "        text_contents = []\n",
    "\n",
    "        for slide in presentation.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text_contents.append(shape.text)\n",
    "\n",
    "        return '\\n'.join(text_contents)\n",
    "\n",
    "\n",
    "def parse_json(json_string):\n",
    "    \"\"\"\n",
    "    Parse a JSON string into a Python object.\n",
    "\n",
    "    Args:\n",
    "        json_string (str): JSON string to parse, optionally with markdown code block markers\n",
    "\n",
    "    Returns:\n",
    "        dict/list: Parsed JSON object\n",
    "\n",
    "    Raises:\n",
    "        json.JSONDecodeError: If the JSON string is invalid\n",
    "    \"\"\"\n",
    "    cleaned_string = json_string.replace('```json', '').replace('```', '').strip()\n",
    "    \n",
    "    return json.loads(cleaned_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Various Image Segmentation Techniques:\\nDifferent types of Image Segmentation Techniques\\nThresholding technique segmentation\\nHistogram based segmentation\\nRegion based segmentation\\nEdge based segmentation\\nClustering based segmentation\\nMorphological Transforms and\\nTexture based segmentation approaches\\nThresholding technique segmentation\\nSegmentation algorithms based on thresholding approach are suitable for images where there is distinct difference between object and background.\\nMain Goal: divide an image into two distinct regions (object and background) directly based on intensity values and their properties\\nTypes: Global, Variable, Multiple\\n\\nOriginal coins image\\n2)Histogram based segmentation\\nHistogram of an image is a plot between intensity levels. \\nDeep valleys are used to separate different peaks of histogram. \\nHistogram peaks are tall, narrow, symmetric.\\n3) Region based Segmentation\\nThe region-based segmentation methods segments the image into various regions having similar characterizes. \\n\\nTwo Techniques: \\na. Region growing, and\\nb. Region split and merge techniques \\nWatershed \\ntransformation\\n\\n4) Edge-based segmentation \\nEdges are defined as “Sudden and Significant changes in the intensity” of an image. \\nBoundaries demonstrates changes between object in the images. \\nSignificant change in intensity between objects in the images\\nUseful when there are many objects in the images. \\nDifferent than region-based segmentation. Edge-based segmentation completely based on dissimilarities or discontinuity. \\n\\n\\n5) Clustering-based segmentation \\nThe clustering-based techniques segment the image into cluster having pixels with similar   characteristics.\\nMany types of clustering. \\nSelect the best of them that proven success rate with the image processing. \\n7) Texture based segmentation approaches\\nSegmentation based on texture characteristics consists of dividing an image into different     regions based on similarity in texture features. \\n Texture = repeated pattern of information\\nTexture image = appearance, structure, and arrangement of the parts of an object within the image. \\n6)Morphological Transforms based Segmentation \\nMorphology is study of shapes.\\nIt has a set of image processing operations that process images based on shapes and not based on pixel intensities. \\nRemember: shapes segmentations = Morphological transforms-based segmentation\\nBasic Morphological Filtering:\\nErosion\\nDilation\\nOpening\\nClosing\\nWhite Tophat\\nBlack Tophat\\nSkeletonize \\nConvex Hull\\n\\nDifferent types of Morphological Filtering\\nSkeleton and Convex hull method: \\nThe convex skull of a polygon is the largest convex polygon contained inside it. It can be found in polynomial time, but the exponent of the algorithm is high.\\nThinning is used to reduce each connected component in a binary image to a single-pixel wide skeleton. It is important to note that this is performed on binary images only.\\nInteraction with 3D images\\n3D image processing is the visualization, processing, and analysis of 3D image data through geometric transformations, filtering, image segmentation, and other morphological operations.\\n3D kidney image\\n\\xa0A slice (2D plane) of the 3D data Kidney image\\n# Animation frames from 3D images \\n3D animation creates moving images in a 3D environment, giving digital objects the illusion of motion through 3D space.\\nTensor Structure computation  of 3D images \\nIn the context of volumetric (3D) image analysis, a structure tensor is a 3-by-3 matrix which summarizes orientation in a certain neighborhood around a certain point.\\nLet’s Go to Exercise ……',\n",
       " 'Soil Mechanics Lectures\\nch-1\\nAsst. Prof. Rakshak Dhungel, nec\\n\\nEvaluation\\nTheory\\nPractical\\nTotal\\nSessional\\n30\\n20\\n50\\nFinal\\n50\\n-\\n50\\nTotal\\n80\\n20\\n100\\n\\nInternal Marks Distribution\\n• Attendance\\n• Tutorials\\n• Lab report preparation\\n• MCQ and Viva                        \\n \\n    \\n• UT and Assessment Exam           \\n\\nText books\\n\\nTextbooks\\n\\nSyllabus\\n• Chapter 1 – Introduction\\n• Chapter 2 – Phase relationship and Index        \\n   \\n     properties of soils\\n• Chapter 3 – Soil Identification and classification\\n• Chapter 4 – Introduction to clay minerals\\n• Chapter 5 – Compaction of soil\\n• Chapter 6 – Effective stress, capillarity and \\n \\n \\n     permeability\\n• Chapter 7 – Seepage analysis through soil\\n\\nSyllabus\\n• Chapter 8 – Vertical stress below applied load\\n• Chapter 9 – Compressibility and consolidation   \\n      of soil\\n• Chapter 10 – Shear strength of soil\\n• Chapter 11 – Stability of slopes\\n\\nPracticals\\n• Particle size distribution test (Sieve Analysis)\\n• Determination of specific gravity of soil by \\npycnometer method.\\n• Determination of Atterberg’s limit of soil\\n    (LL, PL, SL)\\n• Determination of OMC and maximum dry density\\n• Determination of in-situ density by Core cutter \\nmethod and Sand replacement method.\\n• Permeability test\\n• Direct Shear test\\n• One dimensional consolidation test\\n\\nSoil and Rock\\n• The term derived from the Latin word \\n‘Solium’. Solium means upper layer of earth \\ncrust that may be dug or ploughed.\\n• A natural aggregate of mineral particles \\nbonded by strong cohesive force is called \\nrock.\\n\\n• Soil has different interpretation for intellectual \\nbelonging to different disciplines.\\n• The definition given to soil by an agriculturist or \\ngeologist is different from one used by civil \\nengineers.\\n• Agriculturist mean soil as the top layer of earth \\nwhich supports plant life.\\n• Geologist mean soil as thin outer layer of loose \\nsediments in the crust within which roots occur \\nand rest of the crust is grouped under the term \\nrock.\\n\\n• To the civil engineers, soil is any un-cemented or \\nweakly cemented accumulation of solid grain \\nparticles, the void space between the particles \\ncontaining water and/or air.\\n\\uf0d8grains(mineral grains, rock fragments etc)\\n\\uf0d8Void space(water and air)\\n\\uf0d8The void spaces are changed by changes in field \\ncondition.\\nFig: Soil grain structure\\n\\uf0d8Engineers assess soil properties, such as its \\nbearing \\ncapacity, \\ncompaction \\ncharacteristics, \\npermeability, \\nand \\nsettlement \\npotential, \\nto \\ndesign \\nand \\nconstruct structures like buildings, roads, \\nbridges, and dams that are safe and stable \\non the soil beneath them.\\n\\n• Soil consists of a multiphase aggregation of solid \\nparticles, water, and air. \\n• This fundamental composition gives rise to unique \\nengineering properties.\\n• The \\ndescription \\nof \\nits \\nmechanical \\nbehaviour(strength, permeability, seepage etc.) \\nrequires some of the most classic principles of \\nengineering mechanics.\\n• Soil Mechanics is a discipline of civil engineering \\ninvolving \\nthe \\nstudy \\nof \\nsoil \\n(identification, \\nbehaviour and properties) and application as an \\nengineering material.\\n\\n• The scientific study of soil mechanics was first \\nstarted by Karl Terzaghi and hence he is known as \\nfather of soil mechanics.\\n• Soil Mechanics is the application of laws of \\nhydraulics and mechanics to engineering problem \\ndealing with soil particles.\\n• The fundamental principles of soil mechanics is \\ndivided into two parts:-\\n\\uf0d8A description of soil properties.\\n\\uf0d8The application of these properties to solution of \\nsoil engineering problems related to seepage, \\ncompaction, consolidation, bearing capacity etc.\\n\\n\\uf071Soil Behaviour is Complex\\n\\uf0d8Foundation soil has to be accepted as it is at the site\\n\\uf0d8Soil is not coherent material but particulate material\\n\\uf0d8Anisotropic\\n\\uf0d8Non-Homogeneous\\n\\uf0d8Non-Linear\\n\\uf0d8Stress history dependant\\n\\uf071Complexity gives rise to importance of\\n\\uf0d8Theory\\n\\uf0d8Lab Tests\\n\\uf0d8Field Tests\\n\\uf0d8Empirical Relations\\n\\uf0d8Computer applications\\n\\uf0d8Experience, Judgement, FOS\\nSoil Mechanics Review\\n\\n• Soil mechanics equips engineers with scientific tools to \\nunderstand behaviour of soil i.e. response of soils to \\nloads.\\n• A good understanding of soil behaviour is necessary to \\nanalyse and design support system for roads, bridges, \\nbuildings, reservoirs, water distribution system, hydro \\nelectric power stations, nuclear plants etc.\\n• The stability and life of these structures depend on \\nstability, strength and deformation of soils.\\n• If soil fails the structure founded on or within it will fail \\nregardless of how well these system are designed.\\n• Thus successful civil engineering projects are heavily \\ndependant on understanding of soil behaviour.\\nSoil Mechanics Review\\n\\nSOIL PROBLEMS IN CIVIL ENGINEERING\\nTo a civil Engineer soil is a material that can be:\\nbuilt on: foundations to buildings, bridges. \\nbuilt in: tunnels, cross drainage structure, basements. \\nbuilt with: roads, runways, embankments, dams. \\nsupported: retaining walls \\n\\uf0d8A civil engineer has to deal with soil in diverse roles. An engineer while constructing \\nvarious structures must face a number of soil related problems.\\n\\uf0d8The stability of various construction is affected by engineering properties and \\ncharacteristics of soil. \\n\\uf0d8It is necessary to study behavior of the soil and its interactions with the structure to \\nensure its stability. \\n\\nImportance of Soil Mechanics\\n1. Foundations\\n• All foundations for any structure that a civil \\nengineer constructs are bound to rest on the \\nsoil. \\n• The bigger the building or structure, the \\nbigger its foundation \\n• Consequently the more important it is for a \\ncivil engineer to take into consideration the \\nsoil mechanics of the site. \\n\\n• Hard soil with sufficient strength allows an engineer \\nto use shallow foundations, and the weak soil will \\nneed deep foundations to provide robust support for \\nthe structure being constructed.\\n• The choice of foundation is dependent at the depth \\nat which suitable stratum is found.\\nShallow Foundation\\nDeep Foundation\\n\\n2. Earthen Dams\\n• Dam construction provide water for domestic use \\nall year round, support irrigation and also used for \\nhydroelectric power generation.\\n• A proper design is necessary to ensure that they can \\nwithstand the pressure from water and other \\nelements in order to serve their purpose for a long \\ntime without any incidents. \\n• Understanding soil mechanics will ensure that any \\ncivil engineer carrying out such a project takes into \\nconsideration soil properties such as its density, \\npermeability, \\nparticle \\nsize \\ndistribution, \\nconsolidation and compaction characteristics, and \\nshear strength parameter under to come up with a \\nsolid structure. \\n\\n3. Embankments and Excavations\\n• Embankment refers to piling and excavation refers to \\nremoval of huge masses of soil.\\n• Embankments are usually constructed to raise the level \\nof a road, railway or land above ground level or flooding \\nlevel. \\n• Excavation is carried when it is required to construct \\nfoundation, construct basement, construct roads, \\nconstruct canals, lay drainage pipes, construct trench to \\nstudy soil profile, construct borrow pit and to extract \\nresources like oil, coal etc.\\n• Being aware and able to factor things such as slope \\nstability, compaction of soil as well as aspects such as \\neffects of soil seepage, design of support system like \\nbracing system, sheet pile etc. all contribute to stability \\nof embankment and excavation slopes.\\n\\nEmbankment\\nExcavation\\nExcavation for trench\\n\\n4. Retaining structure and Underground structures\\n\\uf0d8Retaining Structure\\n• The role of retaining structure is to retain the soil and \\nprevent it from spreading laterally.\\n• They are commonly used to accommodate changes in \\ngrade, provide increase in right-of-way and buttress the \\ntoe of slopes.\\n• The various types of retaining walls constructed are \\nGravity retaining walls, Cantilever retaining walls, Gabion \\nretaining walls etc.\\n•  The retaining walls, whether made of compacted soil or \\nconcrete, should also be designed accordingly having \\ntaken into consideration the soil mechanics that will be at \\nplay depending on the given surroundings soil type.\\n\\n• The retaining wall should be capable of supporting both the \\nweight of the wall and the force resulting from the earth \\npressure acting upon it without: - \\n\\uf0fcOverturning and \\n\\uf0fcBearing capacity failure \\n\\uf0fcSliding of the wall   \\n\\uf0fcUndue settlement\\n• The reasons for failures might be due to saturated backfill \\nand improper drainage behind the wall (lack of weep holes \\nor clogged holes), foundation problems, settlement or \\nexpansion of the soil, overloading of the wall, construction \\nerrors, lack of passive support, and/or other design errors.\\n\\n\\uf0d8Underground structures\\n• Tunnels, shafts and conduits require evaluation of forces \\nexerted by the soil on these structures.\\n• Today, most urban centres usually construct their infrastructure \\nsuch as gas lines, electricity lines, drainage structures, subways \\nand internet distribution cables underground. \\n• For large urban areas and cities, this can mean digging and \\ntunnelling through miles and miles of heterogeneous soil to be \\nable to reach the millions of houses that make up the city. \\n• The need to understand soil mechanics in such civil engineering \\nworks is more pronounced for these underground projects. \\n• Being able to predict how the soil will behave and affect an \\nunderground pipeline or a subway is important so that the \\ncompleted project can withstand the conditions underground \\nand serve its purpose. \\n\\nFormation of soil\\n• Engineering properties of soils is influenced by how soil is \\nformed.\\n• Knowledge \\nof \\nsoil \\nforming \\nprocess \\ngreatly \\nfacilitate \\nclassification of soil\\n• Exposed rocks are eroded and degraded by various physical and \\nchemical processes like mechanical disintegration, chemical \\ndecomposition or biological weathering.\\n• The products of erosion are picked up and transported to some \\nother place by wind, water etc.\\n• The transported soil are deposited under suitable condition.\\n• Physical and chemical weathering gives two weathering \\nproducts \\n\\uf0d8Mobile ones which are transported over varying distances \\nunder the action of gravity, wind, erosion by water \\n\\uf0d8Residual products that remain on the site of destruction of the \\nparents rocks. \\n\\nFormation of soil\\nA. Physical Weathering\\n• It is natural process of disintegration of rocks into smaller \\nfragments and particles without including any chemical \\nchange in the end product. \\n• Physical weathering is a process of fragmentation of rock \\ndue to some physical forces associated with the factors like \\nfluctuations in temperature, change in the pressure, \\ngrowth of crystals, freezing of water, frost action etc. \\n• The main agents involved are wind, running water and \\nglacier.\\n• The products formed are mainly coarse soils.\\n\\nFormation of soil\\nFactors contributing to Physical weathering \\n• The periodic contraction and expansion of rock \\ndue to variation in temperature, results flaking off \\nof the upper layer of rock causing exfoliations of \\nthe rock \\n• Alternation of light and dark minerals, variation in \\nexpansion leads to development of cracks. \\n• Freezing and thawing of water \\n• Roots of plants \\n• Movement of capillary water \\n• Crystallizations of minerals \\n\\nFormation of soil\\nB. Chemical Weathering\\n• It is the process of alteration of rocks of the earth’s crust by \\nchemical decomposition of rocks \\n• It is due to the effect of : carbonation and dissolution, \\nhydrolysis, oxidation, and hydration. \\n• The end product invariably has a different chemical \\ncomposition and poorer physical constitution as compared to \\nthe parent rock. \\n• It has been observed that water play as an important role in \\nchemical weathering as water can dissolve many active gases \\nfrom the atmosphere such as carbon dioxide, nitrogen, \\nhydrogen etc. \\n• The results of chemical weathering are generally fine soils \\nwith altered mineral grains. \\n\\n\\uf071Carbonation and Dissolution\\n• As rain goes through the air and into the \\nground, it grabs carbon dioxide, creating \\ncarbonic acid. \\n• This weak acid reacts with the calcium \\ncarbonate in rocks and dissolves it when it \\nseeps into the cracks. \\n• CO2 + H2O = H2CO3 \\n• CaCO3 + H2CO3 = 2HCO3\\n- + 2Ca2\\n+ \\n\\n\\uf071Hydrolysis\\n• The term hydrolysis combines the prefix hydro, referring \\nto water, with lysis, which is derived from a Greek word \\nmeaning to loosen or dissolve. \\n• Hydrolysis is a chemical reaction where water loosens the \\nchemical bonds within a mineral and produce new \\ncompounds which tend to be softer and weaker than the \\noriginal parent rock material. \\n• This might sound the same as dissolution but the \\ndifference is that hydrolysis produces a different mineral \\nin addition to ions. \\n• An example of hydrolysis is when water reacts with \\npotassium feldspar to produce clay minerals and ions. \\n2KAlSi3O8 + 3H2O = Al2Si2O5(OH)4 + 4Si02 + 2K+ + 2OH-\\n\\n\\uf071Oxidation\\n• Oxidation is another kind of chemical weathering \\nthat occurs when oxygen combines with another \\nsubstance and creates compounds called oxides.\\n• When rocks, particularly those with iron in them, \\nare exposed to air and water, the iron undergoes \\noxidation, which can weaken the rocks and make \\nthem crumble.\\n• Addition of oxygen results in less stable crystal \\nand \\nthus subjected \\nto \\ndisintegration and \\ndecomposition.\\n\\n\\uf071Hydration\\n• Hydration is a type of chemical weathering \\nwhere water reacts chemically with the rock, \\nmodifying its chemical structure. \\n• A consequence of hydration is that the resulting \\nmineral has a greater volume than the original \\nmineral. \\n• The increase in volume creates stress thus \\napplying force to an overlying layer/underlying \\nlayer and breaking it into pieces. \\n\\nFormation of soil\\nC. Biological Weathering\\n• It is the disintegration or decay of rocks and \\nminerals caused by chemical or physical agents of \\norganisms. \\n• Examples: organic activity from lichen and algae, \\nrock disintegration by plant or root growth, \\nburrowing and tunnelling organisms, and acid \\nsecretion. \\n• Organic soil is formed as a result of biological \\nweathering which is highly compressible and is \\nunsuitable for foundation material.\\n\\n• The soil is called Transported soil if the \\nproducts of rock weathering are transported \\nfrom the place where they originated and re-\\ndeposited to any other place. \\n• The soil is called Residual soil if the products \\nof rock weathering are still located at the \\nplace where they originated.\\nTypes of Soils on basis of origin\\nTypes of Soils on basis of size\\n• Coarse grained soil (> 0.075 mm)\\n• Fine grained soil (< 0.075 mm)\\n\\nTypes of transported soils\\n• Glacial soils: \\n \\nFormed by transportation and deposition of glaciers.\\n• Alluvial soils: \\n \\nTransported by running water and deposited along streams.\\n• Lacustrine soils: \\n \\nFormed by deposition in quiet lakes \\n•  Marine soils: \\n \\nFormed by deposition in the seas \\n• Aeolian soils: \\n \\nTransported and deposited by the wind\\n• Colluvial soils: \\n \\nFormed by movement of soil from its original place by gravity,    \\nsuch as during landslide \\n\\nResidual Soil Vs Transported Soil\\nResidual Soil\\nTransported Soil\\nThese soils are found at the same location \\nwhere they have originated\\nThe rock materials are weathered and \\nthen they are moved from their original \\nlocation to a new place by one or more \\nerosion agents to form transported soils\\nThe parent material is the original \\nbedrock\\nThe parent material is different from the \\nunderlying bedrock\\nThe soil has the minerals similar to those \\nin the bed rock\\nThe soil will have a mineral composition \\ndifferent from that of the bedrock\\n\\nRock Cycle\\n\\n• Exposed rocks eroded and degraded by various \\nphysical and chemical process.\\n• Product of erosion picked up by various agencies of \\ntransportation like wind, water\\n• The erosion products are carried out to new \\nlocations where they get deposited.\\n• This shifting of material causes upheavals at the \\ndeposited location after some time.\\n• The cycle is repeated again\\nSoil Formation cycle\\n',\n",
       " 'Introduction:\\nThis is a sample document created for testing the file reading function. It contains various sections and some sample content to ensure that the function works as expected.\\n\\nSection 1: Overview\\n- This section provides a brief overview of the document.\\n- It explains the purpose of the document and what it aims to achieve.\\n\\nSection 2: Features\\n- Feature 1: Reading text files.\\n- Feature 2: Reading PDF files.\\n- Feature 3: Reading PowerPoint files.\\n\\nSection 3: Implementation\\n- The implementation involves using different libraries to handle each file type.\\n- For text files, the built-in `open` function is used.\\n- For PDF files, the `PyPDF2` library is used.\\n- For PowerPoint files, the `python-pptx` library is used.\\n\\nConclusion:\\nThis document has provided a sample structure and content to test the file reading function. By ensuring that the function can handle different file types, we can verify its robustness and reliability.\\n\\nThank you for using this sample document!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = ['docs/sample.pptx', 'docs/soil.pdf', 'docs/sample.txt']\n",
    "extracted_files = []\n",
    "for file in file_path:\n",
    "    reader = DocumentReader(file)\n",
    "    text = reader.extract_text()\n",
    "    extracted_files.append(text)\n",
    "extracted_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import shutil\n",
    "import yt_dlp\n",
    "from moviepy.editor import AudioFileClip\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Youtube Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_output_directory(output_dir):\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "def download_youtube_video_and_audio(video_url, output_dir='Saved_Media'):\n",
    "    setup_output_directory(output_dir)\n",
    "\n",
    "    # Commented as the video download is not required\n",
    "    # video_options = {\n",
    "    #     'format': 'best',\n",
    "    #     'outtmpl': os.path.join(output_dir, 'video.%(ext)s'),\n",
    "    # }\n",
    "\n",
    "    audio_options = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': os.path.join(output_dir, 'audio.%(ext)s'),\n",
    "    }\n",
    "\n",
    "    # for download_options in [video_options, audio_options]:\n",
    "\n",
    "    with yt_dlp.YoutubeDL(audio_options) as youtube_downloader:\n",
    "        youtube_downloader.download([video_url])\n",
    "\n",
    "    print(\"Video and audio download completed!\")\n",
    "\n",
    "def convert_audio_to_mp3(input_audio_path, output_mp3_path):\n",
    "    audio = AudioFileClip(input_audio_path)\n",
    "    audio.write_audiofile(output_mp3_path)\n",
    "    audio.close()\n",
    "    print(\"Audio conversion to MP3 completed!\")\n",
    "\n",
    "def process_youtube_video(video_url):\n",
    "    output_dir = 'Saved_Media'\n",
    "    \n",
    "    download_youtube_video_and_audio(video_url, output_dir)\n",
    "    \n",
    "    input_audio_path = os.path.join(output_dir, 'audio.webm')\n",
    "    output_mp3_path = os.path.join(output_dir, 'audio.mp3')\n",
    "    \n",
    "    convert_audio_to_mp3(input_audio_path, output_mp3_path)\n",
    "\n",
    "def clean_captions(raw_captions):\n",
    "    lines = raw_captions.decode('utf-8').split('\\n')\n",
    "    cleaned_captions = []\n",
    "    timestamp_pattern = re.compile(r'\\d{2}:\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}:\\d{2}\\.\\d{3}')\n",
    "    \n",
    "    for line in lines:\n",
    "        if line and not line.startswith('WEBVTT') and not timestamp_pattern.match(line):\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line:\n",
    "                cleaned_captions.append(cleaned_line)\n",
    "    \n",
    "    return ' '.join(cleaned_captions)\n",
    "\n",
    "def extract_subtitles(video_url):\n",
    "    subtitle_options = {\n",
    "        'writesubtitles': True,\n",
    "        'subtitleslangs': ['en'],\n",
    "        'subtitlesformat': 'vtt',\n",
    "        'skip_download': True,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(subtitle_options) as youtube_downloader:\n",
    "        info_dict = youtube_downloader.extract_info(video_url, download=False)\n",
    "        subtitles = info_dict.get('requested_subtitles', None)\n",
    "        \n",
    "        if subtitles:\n",
    "            print(f\"Subtitles available for {video_url}\")\n",
    "            for lang, subtitle_info in subtitles.items():\n",
    "                subtitle_url = subtitle_info.get('url')\n",
    "                if subtitle_url:\n",
    "                    response = requests.get(subtitle_url)\n",
    "                    if response.status_code == 200:\n",
    "                        raw_subtitles = response.content\n",
    "                        return clean_captions(raw_subtitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperTranscriber:\n",
    "    def __init__(self, model_name: str = \"openai/whisper-tiny\"):\n",
    "        \"\"\"\n",
    "        Initialize the WhisperTranscriber with a specified model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the Whisper model to use.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.processor = WhisperProcessor.from_pretrained(model_name)\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def load_audio(file_path: str, target_sampling_rate: int = 16000) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"\n",
    "        Load and resample audio to 16kHz if necessary using librosa.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the audio file.\n",
    "            target_sampling_rate (int): Target sampling rate (default: 16000).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, int]: Resampled audio array and sampling rate.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            audio, sr = librosa.load(file_path, sr=target_sampling_rate, mono=True)\n",
    "            return audio, sr\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading audio file: {e}. Make sure the file format is supported by librosa.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def chunk_audio(audio: np.ndarray, chunk_length: int = 30, sampling_rate: int = 16000) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Split audio into chunks for long transcriptions.\n",
    "\n",
    "        Args:\n",
    "            audio (np.ndarray): The audio array.\n",
    "            chunk_length (int): Length of each chunk in seconds (default: 30).\n",
    "            sampling_rate (int): Sampling rate of the audio (default: 16000).\n",
    "\n",
    "        Returns:\n",
    "            List[np.ndarray]: List of audio chunks.\n",
    "        \"\"\"\n",
    "        chunk_size = chunk_length * sampling_rate\n",
    "        return [audio[i:i+chunk_size] for i in range(0, len(audio), chunk_size)]\n",
    "\n",
    "    def transcribe_chunk(self, chunk: np.ndarray, sampling_rate: int = 16000) -> str:\n",
    "        \"\"\"\n",
    "        Transcribe a single audio chunk.\n",
    "\n",
    "        Args:\n",
    "            chunk (np.ndarray): Audio chunk to transcribe.\n",
    "            sampling_rate (int): Sampling rate of the audio (default: 16000).\n",
    "\n",
    "        Returns:\n",
    "            str: Transcribed text.\n",
    "        \"\"\"\n",
    "        input_features = self.processor(chunk, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        input_features = input_features.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = self.model.generate(input_features)\n",
    "\n",
    "        return self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    def transcribe(self, file_path: str, chunk_length: int = 30) -> str:\n",
    "        \"\"\"\n",
    "        Transcribe long audio by chunking.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the audio file.\n",
    "            chunk_length (int): Length of each chunk in seconds (default: 30).\n",
    "\n",
    "        Returns:\n",
    "            str: Full transcription of the audio.\n",
    "        \"\"\"\n",
    "        audio, sampling_rate = self.load_audio(file_path)\n",
    "        chunks = self.chunk_audio(audio, chunk_length, sampling_rate)\n",
    "        \n",
    "        transcriptions = [self.transcribe_chunk(chunk, sampling_rate) for chunk in chunks]\n",
    "        return \" \".join(transcriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3NgmslFf33I\n",
      "[youtube] 3NgmslFf33I: Downloading webpage\n",
      "[youtube] 3NgmslFf33I: Downloading ios player API JSON\n",
      "[youtube] 3NgmslFf33I: Downloading web creator player API JSON\n",
      "[youtube] 3NgmslFf33I: Downloading m3u8 information\n",
      "Getting YouTube Video's Audio...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3NgmslFf33I\n",
      "[youtube] 3NgmslFf33I: Downloading webpage\n",
      "[youtube] 3NgmslFf33I: Downloading ios player API JSON\n",
      "[youtube] 3NgmslFf33I: Downloading web creator player API JSON\n",
      "[youtube] 3NgmslFf33I: Downloading m3u8 information\n",
      "[info] 3NgmslFf33I: Downloading 1 format(s): 251-5\n",
      "[download] Destination: Saved_Media\\audio.webm\n",
      "[download] 100% of    7.66MiB in 00:00:04 at 1.66MiB/s   \n",
      "Video and audio download completed!\n",
      "MoviePy - Writing audio in Saved_Media\\audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio conversion to MP3 completed!\n",
      "Transcribing Audio...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi, welcome to another video. So Google has launched their new Gemini 2.0 flash model. Yes, the Gemini 2.0 flash model is now available for users to use. It is initially marked as experimental, but this is majorly the final form of this model. And this is the first frontier model in their two points.  0 lineup. To be honest, it's the first next gen model that we are seeing since OpenAI is still stuck on GPT4 and clawed on 3.5 so on it. But this is the first new frontier model by Google. They say that is twice as fast as pro while achieving stronger performance. The major part about this model is that it comes with native multi-modal outputs and native tool use.  Basically, Gemini 2.0 flash can now generate and edit images in itself without hitting any other end point. It can also generate audio outputs as well within itself, which is just amazing. It's like GPT40, but it can also generate images, which is pretty amazing. They say that Gemini 2.0 flash features native text to speech audio output that provides developers  FindGrainDControl over not just what the model says, but how it says it with a choice of eight high-quality voices and a range of languages and accents. It can also now do native image output and they say that Gemini 2.0 flash now natively generates images and supports conversational, multi-turn editing. So you can build on previous outputs and refine them. It can output  interleaved text and images, making it useful in multi-modal content such as recipes. So, it can generate audio and images as well. It is also natively trained for tool use, which makes it much better performing in agentic tasks that require tool calling. There's also a new multi-modal API, which basically allows you to do the same things as GPT-4-Os real-time API.  But this is by Gemini and has a free tier. The free tier remains the same and gives you the same stuff as the previous Gemini 1.5 flash, which is great because I can never get over the rate limit even if I try hard. So, it's essentially free. I'll come to the benchmarks as well about the model, but there's another thing that they have announced and it's their new AI coding agent called Jules.  It is an AI coding agent to which you can give any task and it can do that in your code base. This coding agent is super cool. It can asynchronously integrate with your GitHub workflow. It handles bug fixes and other time-consuming tasks. It creates comprehensive, multi-step plans to address  issues, efficiently modifies multiple files, and even prepares pull requests to land fixes directly back into GitHub. It is under a wait list, and people who register will be given access next year or next month. However, you like to say that. Anyway, that seems to be super cool as well. I don't think it will cost $500 like Devon. So, that should be cool. Let's take a look at the  benchmarks now. So, first of all, they haven't compared it to any other models apart from their own, which I find is fine, because it's better to compare with what is fair. Anyway, Gemini 2.0 flash outperforms 1.5 Pro 2 in all benchmarks. In coding, it scores much higher than Pro, which is great. Video and long context is where it struggles to catch up with Pro,  But that's fine. There's also a project mariner that they are developing apart from this, where they were able to take Gemini Flash and make it control whole web browsers and stuff like that. It's under research and they'll probably release it somewhere next year if they see it fit. So, this is the first model in the Gemini 2.0 family. Now, let me show you how you can use it. So, first of all,  You can just go to Google AI Studio. And here, you can see the Gemini 2.0 Flash Experimental Model. It comes under the same rate limits as Flash, which is great. You can also see two new options here. The first one is the Stream Real Time option, which is basically the same as GPT40, where you can start a real-time talk with Gemini 2, or you can also share your screen and everything like that.  Now, I tried to make it generate or edit images, but it wasn't able to do that in the Google AI Studio. In many cases, it says that it is doing it, but then the image never appears, and it doesn't work. Maybe it's a glitch or something, because they did indeed show it in their demo, but it doesn't work for me. Some people on the internet are also reporting the same issue. So, that needs to be fixed.  Also, the image generation thing is basically image editing. You cannot make images from scratch. You'll need to give it an image, and then it can change it accordingly based on the prompt. Anyway, it doesn't work for now. In the real time thing, you can try it out as well. You select the modality that you want to use, like text to audio or audio to audio, or everything with screen sharing and  So that's super good as well. Let me send a simple prompt of what is bigger? 9.9 or 9.11. So here's the answer and you can see because I chose audio, I got an audio stream here and you can hear it.  to write. Since the whole number part is equal in both numbers, we look at the tense place, which is 9 in 9.9 and 1 in 9.11. 9 is bigger than 1, so 9.9 is bigger.  which is also pretty amazing to be honest. Another thing that they have is some example apps. Here, you can see some example apps like this, spatial understanding one. Here, you can upload an image and ask it to generate bounding boxes over them. So, there's this example image, and if we send it, then you can see that it makes the API request here. And if we wait a bit, then you can see the bounding boxes  over each element here. These tools are shared to show the agentic capabilities of the new Flash model, which is great. I also did my question testing on it as well, and it nailed all the questions except one, which is expected. But this is great. It's probably very close to Sonnet, while being free for the API usage, which is just insane to me. The multi-model thing is also great with it.  Many people want to see the butterfly that it creates. So this is the butterfly that it created and to be honest, it's one of the best ones that I have seen. So that's super cool. To be honest, this model is really great and I'm excited to bring some great videos with it and this just makes much more great AI things accessible to free users like us. This is just insane to be honest and it's not even the pro model.  It's just the flash model. At one place, people are paying $200 to 01 and here Google is coming up with the best frontier models. I'll be bringing some great videos with this model. So stay tuned for that. Overall, it's pretty cool. Anyway, let me know your thoughts in the comments. If you liked this video, consider donating to my channel through the Super Thanks option below.  consider becoming a member by clicking the join button. Also, give this video a thumbs up and subscribe to my channel. I'll see you in the next video. Till then, bye.\n"
     ]
    }
   ],
   "source": [
    "def get_youtube_transcript(youtube_url: str) -> str:\n",
    "    try:\n",
    "        existing_subtitles = extract_subtitles(youtube_url)\n",
    "        if existing_subtitles:\n",
    "            print(\"Subtitles found ... Extracted Subtitles from YouTube Video...\")\n",
    "            return existing_subtitles\n",
    "        else:\n",
    "            print(\"Getting YouTube Video's Audio...\")\n",
    "            process_youtube_video(youtube_url)\n",
    "            audio_file = r\"Saved_Media\\audio.mp3\"\n",
    "            if not os.path.exists(audio_file):\n",
    "                raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n",
    "            \n",
    "            print(\"Transcribing Audio...\")\n",
    "            transcriber = WhisperTranscriber()\n",
    "            transcribed_text = transcriber.transcribe(audio_file)\n",
    "            return transcribed_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "youtube_url = \"https://www.youtube.com/watch?v=3NgmslFf33I\"\n",
    "transcript = get_youtube_transcript(youtube_url)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medium Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def get_medium_article_content(url):\n",
    "    try:\n",
    "        # Send request to the URL\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find article content\n",
    "        article_content = \"\"\n",
    "        \n",
    "        # Get title\n",
    "        title = soup.find('h1')\n",
    "        if title:\n",
    "            article_content += f\"Title: {title.get_text()}\\n\\n\"\n",
    "        \n",
    "        # Get main article content\n",
    "        article = soup.find('article')\n",
    "        if article:\n",
    "            # Get all paragraphs\n",
    "            paragraphs = article.find_all('p')\n",
    "            for p in paragraphs:\n",
    "                article_content += f\"{p.get_text()}\\n\\n\"\n",
    "        \n",
    "        return article_content.strip()\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching the article: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing the article: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: Canvas with React.js\\n\\nLucas Miranda\\n\\nFollow\\n\\n--\\n\\n12\\n\\nListen\\n\\nShare\\n\\nIn this article, we will see how to create a Canvas React component and a custom hook for extracting its logic, so we can just draw inside it like we usually draw in a regular canvas html element.\\n\\nThis article is based on Corey’s article “Animating a Canvas with React Hooks”. Any other sources and related contents are linked throughout this article.\\n\\nI am assuming that you already know canvas, but if you don’t know yet, I recommend this tutorial from MDN to you.\\n\\nIn order to see what we are doing, let’s create a new react app with create-react-app (feel free to skip this step if you are already familiar with React and create-react-app). You can start a new project by running npx create-react-app example or yarn create react-app example if you prefer yarn. If you open the project folder (example) in your code editor, you must get something like this:\\n\\nWe don’t need all these files, so we will delete almost everything from src folder and public folder. We just need the index.html, App.js and index.js like you can see there:\\n\\nWe will also keep only the essential in the index.js script and index.html markup file.\\n\\nIf we run yarn start or npm start we can see the “hello world” in the page. So, it is working!\\n\\nNow that we see it is all working, we can replace the “hello world” with the Canvas component that we will create.\\n\\nOur app crash, but it will get fixed when we create the Canvas component.\\n\\nWe need a canvas element to draw inside it, so we must create a component that is basically a canvas element:\\n\\nNow we have a canvas element wrapped in a react component called Canvas. Great! However, how can we draw in it? Well… we will need to get the DOM canvas element itself to get its context object, right? The React way to get a dom element is by giving it a ref prop.\\n\\nTo get the canvas element, we will create a ref and give it to the canvas element:\\n\\nWe can access the canvas element through the canvasRef now. Now we just need to get the context and start drawing! 🙌\\n\\nWhat? Is it broken? Cannot read property getContext of null? getContext is not a function? 😓\\n\\nThe component is not mounted yet when we tried to get the canvas through the ref, so its value is, naturally, the initial value that we gave for it (which is null in my case). We must wait the component did mount properly before get the real canvas. Fortunately, there is a hook to handle that problem!\\n\\nThe useEffect hook allow us to perform side effects in function components. It means that we can call functions right after the component did mount, component update or change of some variable, and some other stuff. (Learn more about useEffect hook here and life cycle of components here)\\n\\nWe are interested in the first case right now: the component did mount. Right after the canvas element is available in the dom for us, we want to get it on javascript to take its context and make some draw. To do that, we pass a function to be executed as the first argument of the useEffect, and an empty array as the second. The empty array says to useEffect that we want execute that function only once, after the component did mount (we will discuss more about this array later). If we pass only the first argument (the function), useEffect will call the function after every single update of the component.\\n\\nNow we are able to draw in the canvas! 🎉🎊🎉🎊🎉\\n\\nWe have a black rectangle in our canvas right now, but we don’t wanna draw a black rectangle every single time we use this component, right? Then, we can take a callback function that make our draw, so our component can be more dynamic. Let’s do it!\\n\\nYou may notice that now we have changed the array as the second argument of the useEffect, right? Now it is no longer empty. We have put the draw function inside it. Do you remember that I said that useEffect could call functions after a change of some variable? This is the case. That array is known as the dependencies array, and everything we put in it, is watched by the useEffect. When anything that is inside of the dependencies array changes, the function will be called again with its updated values. Thus, every time we change the draw, the function of the useEffect will be called again for the new draw.\\n\\nWe have a draw function instead of a bunch of code spread in our useEffect callback now. Nevertheless, we still have a screen with a static draw on it. How about adding some animation to it?\\n\\nNow we have a simple animation!\\n\\nLet me explain what happened here:\\n\\nLooks good, don’t you think? But defining the draw function inside the component doesn’t sounds good… I mean, we still have the same draw for every canvas! 🤔\\n\\nI can see you yelling to me: “TAKE THE DRAW CALLBACK FROM PROPS!!”\\n\\nKeep calm young one, that is exactly what we’re gonna do right now 😎\\n\\nWe should insert the draw function in the Canvas component right now:\\n\\nIt looks great now 🧐. All we need is to use the Canvas component giving the draw function as a prop to it and we have a complete canvas animated.\\n\\nBut… What if we want to make a different component, with some little changes, but keeping the same logic for the canvas? To achieve that, we should create a custom hook for our canvas, then we can use the logic in our canvas, and if we want to create some different component, but keeping the logic, we can just call the hook for handle the logic for us.\\n\\nNote that the only thing we need to give to our logic is the draw callback, and the only thing we have to pass to our component is the canvasRef. Therefore, our component must be something like that in the end:\\n\\nThe draw callback is given to our hook and a ref for our canvas element is returned. Now we just have to copy our logic to the useCanvas hook and transform it in a function that receives the draw callback and return the canvasRef:\\n\\nNow we are (almost) done!\\n\\nWhat if you want to use another context instead of 2d in the canvas? If you have some other configurations that you want for your canvas? Then you might want make the Canvas component more flexible by giving it one more prop: options.\\n\\nThe options prop might be an object which contains all your custom setup for the canvas and whatever you want to your Canvas component. Your canvas hook also must have a little modification ir order to accept new arguments.\\n\\nMaking the code falls back to the tradicional setup will provide a better usability. With that you can setup a webgl context.\\n\\nAt this moment, we don’t have control over the canvas size. What happen if we want a bigger or a smaller canvas? Or even a canvas with dynamic adjustable height and width? We should have a function to handle that.\\n\\nHere I will show how you could track the canvas size, but you can come with a different approach that you prefer for this function.\\n\\nThis way you are able to set the size of your canvas using only CSS, and your draw will not look distorted. You can call this function before the draw function or even inside it.\\n\\nYou may think that the resize function isn’t doing anything at all, but without it your canvas will keep the same initial logical size, therefore, the space used to draw (canvas logical size) can be bigger or smaller than the size being displayed (canvas style size) and you will have trouble because canvas will scale the draw to fit the whole displayed canvas. You can learn more about logical size and CSS size of the canvas here and here.\\n\\nWhen your canvas is running in some device with high pixel density like modern smartphones, the draw can look blurry. To avoid that, you must define the size of your canvas according on the device pixel ratio.\\n\\nHere we rewrite the resize canvas function to take into account the device pixel ratio. It is necessary to scale to the ratio to draw with the actual CSS pixels. Learn more about correcting resolution in a canvas here.\\n\\nIn the draw function, there is some procedures that we might want execute for every animation, like clear the canvas or increment the frame counter. We could abstract this procedures to special functions that will be executed before and after the draw: the predraw and postdraw functions. This way we can write less code every time we want to create a different animation. These functions must be called (obviously) inside of the render function, before and after the draw function.\\n\\nWe could just drop the code of these functions in the render function, but writing in functions will allow us to replace them by taking them from props in the component like that:\\n\\nNote that you still should make some modification in the useCanvas hook in order to accept the predraw and postdraw function. You can modularize it in the way that attends you!\\n\\nWell, that is it! Now you have what is needed to make a complete Canvas component with custom hooks for making animations, static draws or even mini-games. 🎉🎊🎊🎉✨🏆'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_medium_article_content('https://medium.com/@pdx.lucasm/canvas-with-react-js-32e133c05258')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged Code for all total Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict,Any\n",
    "import os\n",
    "import json\n",
    "import fitz\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pptx import Presentation\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Union, Dict, Any\n",
    "from transcribe import WhisperTranscriber\n",
    "from utils import *\n",
    "def get_youtube_transcript(youtube_url: str) -> str:\n",
    "    try:\n",
    "        existing_subtitles = extract_subtitles(youtube_url)\n",
    "        if existing_subtitles:\n",
    "            print(\"Subtitles found ... Extracted Subtitles from YouTube Video...\")\n",
    "            return existing_subtitles\n",
    "        else:\n",
    "            print(\"Getting YouTube Video's Audio...\")\n",
    "            process_youtube_video(youtube_url)\n",
    "            audio_file = r\"Saved_Media\\audio.mp3\"\n",
    "            if not os.path.exists(audio_file):\n",
    "                raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n",
    "            \n",
    "            print(\"Transcribing Audio...\")\n",
    "            transcriber = WhisperTranscriber()\n",
    "            transcribed_text = transcriber.transcribe(audio_file)\n",
    "            return transcribed_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "class DocumentExtractor:\n",
    "    \"\"\"A unified class for extracting content from various document types including web content.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the document extractor with necessary components.\"\"\"\n",
    "        self.youtube_transcriber = WhisperTranscriber()\n",
    "        \n",
    "    def extract_content(self, source: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract content from various document types including web links.\n",
    "        \n",
    "        Args:\n",
    "            source (str): File path or URL to the content\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Dictionary containing:\n",
    "                - 'content': The extracted text/content\n",
    "                - 'metadata': Additional information about the extraction\n",
    "                - 'type': The type of content that was processed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if source is a URL\n",
    "            if source.startswith(('http://', 'https://')):\n",
    "                if 'youtube.com' in source or 'youtu.be' in source:\n",
    "                    return self._extract_youtube_content(source)\n",
    "                elif 'medium.com' in source:\n",
    "                    return self._extract_medium_content(source)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported URL type\")\n",
    "            \n",
    "            # Handle local files\n",
    "            if not os.path.exists(source):\n",
    "                raise FileNotFoundError(f\"The file {source} does not exist.\")\n",
    "                \n",
    "            file_extension = os.path.splitext(source)[1].lower()\n",
    "            \n",
    "            handlers = {\n",
    "                '.txt': self._extract_text_file,\n",
    "                '.pdf': self._extract_pdf,\n",
    "                '.pptx': self._extract_pptx\n",
    "            }\n",
    "            \n",
    "            handler = handlers.get(file_extension)\n",
    "            if not handler:\n",
    "                raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "                \n",
    "            return handler(source)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'content': '',\n",
    "                'metadata': {'error': str(e)},\n",
    "                'type': 'error'\n",
    "            }\n",
    "    \n",
    "    def _extract_text_file(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract content from text files.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return {\n",
    "            'content': content,\n",
    "            'metadata': {'file_size': os.path.getsize(file_path)},\n",
    "            'type': 'text'\n",
    "        }\n",
    "    \n",
    "    def _extract_pdf(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract content from PDF files with OCR support.\"\"\"\n",
    "        doc = fitz.open(file_path)\n",
    "        text_contents = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text = page.get_text(\"text\")\n",
    "            \n",
    "            if not text.strip():\n",
    "                # Try OCR if no text is found\n",
    "                zoom = 2.0\n",
    "                matrix = fitz.Matrix(zoom, zoom)\n",
    "                pixmap = page.get_pixmap(matrix=matrix)\n",
    "                image = Image.frombytes(\"RGB\", [pixmap.width, pixmap.height], pixmap.samples)\n",
    "                text = pytesseract.image_to_string(image)\n",
    "                \n",
    "            text_contents.append(text)\n",
    "            \n",
    "        return {\n",
    "            'content': '\\n'.join(text_contents),\n",
    "            'metadata': {'pages': len(doc)},\n",
    "            'type': 'pdf'\n",
    "        }\n",
    "    \n",
    "    def _extract_pptx(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract content from PowerPoint presentations.\"\"\"\n",
    "        presentation = Presentation(file_path)\n",
    "        text_contents = []\n",
    "        \n",
    "        for slide in presentation.slides:\n",
    "            slide_text = []\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    slide_text.append(shape.text)\n",
    "            text_contents.append(' '.join(slide_text))\n",
    "            \n",
    "        return {\n",
    "            'content': '\\n'.join(text_contents),\n",
    "            'metadata': {'slides': len(presentation.slides)},\n",
    "            'type': 'pptx'\n",
    "        }\n",
    "    \n",
    "    def _extract_youtube_content(self, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract transcript from YouTube videos.\"\"\"\n",
    "        try:\n",
    "            transcript = get_youtube_transcript(url)\n",
    "            return {\n",
    "                'content': transcript,\n",
    "                'metadata': {'url': url},\n",
    "                'type': 'youtube'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to extract YouTube content: {str(e)}\")\n",
    "    \n",
    "    def _extract_medium_content(self, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract content from Medium articles.\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract title\n",
    "            title = soup.find('h1').get_text() if soup.find('h1') else ''\n",
    "            \n",
    "            # Extract article content\n",
    "            article_content = []\n",
    "            content_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "            \n",
    "            for element in content_elements:\n",
    "                text = element.get_text().strip()\n",
    "                if text:\n",
    "                    article_content.append(text)\n",
    "            \n",
    "            return {\n",
    "                'content': '\\n'.join(article_content),\n",
    "                'metadata': {\n",
    "                    'title': title,\n",
    "                    'url': url\n",
    "                },\n",
    "                'type': 'medium'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to extract Medium content: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Processed docs/sample.txt:\n",
      "Type: text\n",
      "Content length: 990 characters\n",
      "Metadata: {'file_size': 1011}\n",
      "\n",
      "Processed docs/soil.pdf:\n",
      "Type: pdf\n",
      "Content length: 15926 characters\n",
      "Metadata: {'pages': 38}\n",
      "\n",
      "Processed docs/sample.pptx:\n",
      "Type: pptx\n",
      "Content length: 3566 characters\n",
      "Metadata: {'slides': 14}\n",
      "\n",
      "Processed https://medium.com/@pdx.lucasm/canvas-with-react-js-32e133c05258:\n",
      "Type: medium\n",
      "Content length: 9166 characters\n",
      "Metadata: {'title': 'Canvas with React.js', 'url': 'https://medium.com/@pdx.lucasm/canvas-with-react-js-32e133c05258'}\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3NgmslFf33I\n",
      "[youtube] 3NgmslFf33I: Downloading webpage\n",
      "[youtube] 3NgmslFf33I: Downloading ios player API JSON\n",
      "[youtube] 3NgmslFf33I: Downloading web creator player API JSON\n",
      "[youtube] 3NgmslFf33I: Downloading m3u8 information\n",
      "Getting YouTube Video's Audio...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3NgmslFf33I\n",
      "[youtube] 3NgmslFf33I: Downloading webpage\n",
      "[youtube] 3NgmslFf33I: Downloading ios player API JSON\n",
      "[youtube] 3NgmslFf33I: Downloading web creator player API JSON\n",
      "[youtube] 3NgmslFf33I: Downloading m3u8 information\n",
      "[info] 3NgmslFf33I: Downloading 1 format(s): 251-5\n",
      "[download] Destination: Saved_Media\\audio.webm\n",
      "[download] 100% of    7.66MiB in 00:00:08 at 939.75KiB/s \n",
      "Video and audio download completed!\n",
      "MoviePy - Writing audio in Saved_Media\\audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio conversion to MP3 completed!\n",
      "Transcribing Audio...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed https://www.youtube.com/watch?v=3NgmslFf33I:\n",
      "Type: youtube\n",
      "Content length: 6953 characters\n",
      "Metadata: {'url': 'https://www.youtube.com/watch?v=3NgmslFf33I'}\n"
     ]
    }
   ],
   "source": [
    "extractor = DocumentExtractor()\n",
    "    \n",
    "sources = [\n",
    "        'docs/sample.txt',\n",
    "        'docs/soil.pdf',\n",
    "        'docs/sample.pptx',\n",
    "        'https://medium.com/@pdx.lucasm/canvas-with-react-js-32e133c05258',\n",
    "        'https://www.youtube.com/watch?v=3NgmslFf33I'\n",
    "    ]\n",
    "\n",
    "for source in sources:\n",
    "    try:\n",
    "        result = extractor.extract_content(source)\n",
    "        print(f\"\\nProcessed {source}:\")\n",
    "        print(f\"Type: {result['type']}\")\n",
    "        print(f\"Content length: {len(result['content'])} characters\")\n",
    "        print(f\"Metadata: {result['metadata']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {source}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \" Hi, welcome to another video. So Google has launched their new Gemini 2.0 flash model. Yes, the Gemini 2.0 flash model is now available for users to use. It is initially marked as experimental, but this is majorly the final form of this model. And this is the first frontier model in their two points.  0 lineup. To be honest, it's the first next gen model that we are seeing since OpenAI is still stuck on GPT4 and clawed on 3.5 so on it. But this is the first new frontier model by Google. They say that is twice as fast as pro while achieving stronger performance. The major part about this model is that it comes with native multi-modal outputs and native tool use.  Basically, Gemini 2.0 flash can now generate and edit images in itself without hitting any other end point. It can also generate audio outputs as well within itself, which is just amazing. It's like GPT40, but it can also generate images, which is pretty amazing. They say that Gemini 2.0 flash features native text to speech audio output that provides developers  FindGrainDControl over not just what the model says, but how it says it with a choice of eight high-quality voices and a range of languages and accents. It can also now do native image output and they say that Gemini 2.0 flash now natively generates images and supports conversational, multi-turn editing. So you can build on previous outputs and refine them. It can output  interleaved text and images, making it useful in multi-modal content such as recipes. So, it can generate audio and images as well. It is also natively trained for tool use, which makes it much better performing in agentic tasks that require tool calling. There's also a new multi-modal API, which basically allows you to do the same things as GPT-4-Os real-time API.  But this is by Gemini and has a free tier. The free tier remains the same and gives you the same stuff as the previous Gemini 1.5 flash, which is great because I can never get over the rate limit even if I try hard. So, it's essentially free. I'll come to the benchmarks as well about the model, but there's another thing that they have announced and it's their new AI coding agent called Jules.  It is an AI coding agent to which you can give any task and it can do that in your code base. This coding agent is super cool. It can asynchronously integrate with your GitHub workflow. It handles bug fixes and other time-consuming tasks. It creates comprehensive, multi-step plans to address  issues, efficiently modifies multiple files, and even prepares pull requests to land fixes directly back into GitHub. It is under a wait list, and people who register will be given access next year or next month. However, you like to say that. Anyway, that seems to be super cool as well. I don't think it will cost $500 like Devon. So, that should be cool. Let's take a look at the  benchmarks now. So, first of all, they haven't compared it to any other models apart from their own, which I find is fine, because it's better to compare with what is fair. Anyway, Gemini 2.0 flash outperforms 1.5 Pro 2 in all benchmarks. In coding, it scores much higher than Pro, which is great. Video and long context is where it struggles to catch up with Pro,  But that's fine. There's also a project mariner that they are developing apart from this, where they were able to take Gemini Flash and make it control whole web browsers and stuff like that. It's under research and they'll probably release it somewhere next year if they see it fit. So, this is the first model in the Gemini 2.0 family. Now, let me show you how you can use it. So, first of all,  You can just go to Google AI Studio. And here, you can see the Gemini 2.0 Flash Experimental Model. It comes under the same rate limits as Flash, which is great. You can also see two new options here. The first one is the Stream Real Time option, which is basically the same as GPT40, where you can start a real-time talk with Gemini 2, or you can also share your screen and everything like that.  Now, I tried to make it generate or edit images, but it wasn't able to do that in the Google AI Studio. In many cases, it says that it is doing it, but then the image never appears, and it doesn't work. Maybe it's a glitch or something, because they did indeed show it in their demo, but it doesn't work for me. Some people on the internet are also reporting the same issue. So, that needs to be fixed.  Also, the image generation thing is basically image editing. You cannot make images from scratch. You'll need to give it an image, and then it can change it accordingly based on the prompt. Anyway, it doesn't work for now. In the real time thing, you can try it out as well. You select the modality that you want to use, like text to audio or audio to audio, or everything with screen sharing and  So that's super good as well. Let me send a simple prompt of what is bigger? 9.9 or 9.11. So here's the answer and you can see because I chose audio, I got an audio stream here and you can hear it.  to write. Since the whole number part is equal in both numbers, we look at the tense place, which is 9 in 9.9 and 1 in 9.11. 9 is bigger than 1, so 9.9 is bigger.  which is also pretty amazing to be honest. Another thing that they have is some example apps. Here, you can see some example apps like this, spatial understanding one. Here, you can upload an image and ask it to generate bounding boxes over them. So, there's this example image, and if we send it, then you can see that it makes the API request here. And if we wait a bit, then you can see the bounding boxes  over each element here. These tools are shared to show the agentic capabilities of the new Flash model, which is great. I also did my question testing on it as well, and it nailed all the questions except one, which is expected. But this is great. It's probably very close to Sonnet, while being free for the API usage, which is just insane to me. The multi-model thing is also great with it.  Many people want to see the butterfly that it creates. So this is the butterfly that it created and to be honest, it's one of the best ones that I have seen. So that's super cool. To be honest, this model is really great and I'm excited to bring some great videos with it and this just makes much more great AI things accessible to free users like us. This is just insane to be honest and it's not even the pro model.  It's just the flash model. At one place, people are paying $200 to 01 and here Google is coming up with the best frontier models. I'll be bringing some great videos with this model. So stay tuned for that. Overall, it's pretty cool. Anyway, let me know your thoughts in the comments. If you liked this video, consider donating to my channel through the Super Thanks option below.  consider becoming a member by clicking the join button. Also, give this video a thumbs up and subscribe to my channel. I'll see you in the next video. Till then, bye.\",\n",
       " 'metadata': {'url': 'https://www.youtube.com/watch?v=3NgmslFf33I'},\n",
       " 'type': 'youtube'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pipeline import DocumentExtractor\n",
    "extractor = DocumentExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3NgmslFf33I\n",
      "[youtube] 3NgmslFf33I: Downloading webpage\n",
      "[youtube] 3NgmslFf33I: Downloading ios player API JSON\n",
      "[youtube] 3NgmslFf33I: Downloading web creator player API JSON\n",
      "[youtube] 3NgmslFf33I: Downloading m3u8 information\n",
      "Getting YouTube Video's Audio...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3NgmslFf33I\n",
      "[youtube] 3NgmslFf33I: Downloading webpage\n",
      "[youtube] 3NgmslFf33I: Downloading ios player API JSON\n",
      "[youtube] 3NgmslFf33I: Downloading web creator player API JSON\n",
      "[youtube] 3NgmslFf33I: Downloading m3u8 information\n",
      "[info] 3NgmslFf33I: Downloading 1 format(s): 251-5\n",
      "[download] Destination: Saved_Media\\audio.webm\n",
      "[download] 100% of    7.66MiB in 00:00:05 at 1.31MiB/s   \n",
      "Video and audio download completed!\n",
      "MoviePy - Writing audio in Saved_Media\\audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio conversion to MP3 completed!\n",
      "Transcribing Audio...\n",
      "Using device: cuda\n",
      "{'content': \" Hi, welcome to another video. So Google has launched their new Gemini 2.0 flash model. Yes, the Gemini 2.0 flash model is now available for users to use. It is initially marked as experimental, but this is majorly the final form of this model. And this is the first frontier model in their two points.  0 lineup. To be honest, it's the first next gen model that we are seeing since OpenAI is still stuck on GPT4 and clawed on 3.5 so on it. But this is the first new frontier model by Google. They say that is twice as fast as pro while achieving stronger performance. The major part about this model is that it comes with native multi-modal outputs and native tool use.  Basically, Gemini 2.0 flash can now generate and edit images in itself without hitting any other end point. It can also generate audio outputs as well within itself, which is just amazing. It's like GPT40, but it can also generate images, which is pretty amazing. They say that Gemini 2.0 flash features native text to speech audio output that provides developers  FindGrainDControl over not just what the model says, but how it says it with a choice of eight high-quality voices and a range of languages and accents. It can also now do native image output and they say that Gemini 2.0 flash now natively generates images and supports conversational, multi-turn editing. So you can build on previous outputs and refine them. It can output  interleaved text and images, making it useful in multi-modal content such as recipes. So, it can generate audio and images as well. It is also natively trained for tool use, which makes it much better performing in agentic tasks that require tool calling. There's also a new multi-modal API, which basically allows you to do the same things as GPT-4-Os real-time API.  But this is by Gemini and has a free tier. The free tier remains the same and gives you the same stuff as the previous Gemini 1.5 flash, which is great because I can never get over the rate limit even if I try hard. So, it's essentially free. I'll come to the benchmarks as well about the model, but there's another thing that they have announced and it's their new AI coding agent called Jules.  It is an AI coding agent to which you can give any task and it can do that in your code base. This coding agent is super cool. It can asynchronously integrate with your GitHub workflow. It handles bug fixes and other time-consuming tasks. It creates comprehensive, multi-step plans to address  issues, efficiently modifies multiple files, and even prepares pull requests to land fixes directly back into GitHub. It is under a wait list, and people who register will be given access next year or next month. However, you like to say that. Anyway, that seems to be super cool as well. I don't think it will cost $500 like Devon. So, that should be cool. Let's take a look at the  benchmarks now. So, first of all, they haven't compared it to any other models apart from their own, which I find is fine, because it's better to compare with what is fair. Anyway, Gemini 2.0 flash outperforms 1.5 Pro 2 in all benchmarks. In coding, it scores much higher than Pro, which is great. Video and long context is where it struggles to catch up with Pro,  But that's fine. There's also a project mariner that they are developing apart from this, where they were able to take Gemini Flash and make it control whole web browsers and stuff like that. It's under research and they'll probably release it somewhere next year if they see it fit. So, this is the first model in the Gemini 2.0 family. Now, let me show you how you can use it. So, first of all,  You can just go to Google AI Studio. And here, you can see the Gemini 2.0 Flash Experimental Model. It comes under the same rate limits as Flash, which is great. You can also see two new options here. The first one is the Stream Real Time option, which is basically the same as GPT40, where you can start a real-time talk with Gemini 2, or you can also share your screen and everything like that.  Now, I tried to make it generate or edit images, but it wasn't able to do that in the Google AI Studio. In many cases, it says that it is doing it, but then the image never appears, and it doesn't work. Maybe it's a glitch or something, because they did indeed show it in their demo, but it doesn't work for me. Some people on the internet are also reporting the same issue. So, that needs to be fixed.  Also, the image generation thing is basically image editing. You cannot make images from scratch. You'll need to give it an image, and then it can change it accordingly based on the prompt. Anyway, it doesn't work for now. In the real time thing, you can try it out as well. You select the modality that you want to use, like text to audio or audio to audio, or everything with screen sharing and  So that's super good as well. Let me send a simple prompt of what is bigger? 9.9 or 9.11. So here's the answer and you can see because I chose audio, I got an audio stream here and you can hear it.  to write. Since the whole number part is equal in both numbers, we look at the tense place, which is 9 in 9.9 and 1 in 9.11. 9 is bigger than 1, so 9.9 is bigger.  which is also pretty amazing to be honest. Another thing that they have is some example apps. Here, you can see some example apps like this, spatial understanding one. Here, you can upload an image and ask it to generate bounding boxes over them. So, there's this example image, and if we send it, then you can see that it makes the API request here. And if we wait a bit, then you can see the bounding boxes  over each element here. These tools are shared to show the agentic capabilities of the new Flash model, which is great. I also did my question testing on it as well, and it nailed all the questions except one, which is expected. But this is great. It's probably very close to Sonnet, while being free for the API usage, which is just insane to me. The multi-model thing is also great with it.  Many people want to see the butterfly that it creates. So this is the butterfly that it created and to be honest, it's one of the best ones that I have seen. So that's super cool. To be honest, this model is really great and I'm excited to bring some great videos with it and this just makes much more great AI things accessible to free users like us. This is just insane to be honest and it's not even the pro model.  It's just the flash model. At one place, people are paying $200 to 01 and here Google is coming up with the best frontier models. I'll be bringing some great videos with this model. So stay tuned for that. Overall, it's pretty cool. Anyway, let me know your thoughts in the comments. If you liked this video, consider donating to my channel through the Super Thanks option below.  consider becoming a member by clicking the join button. Also, give this video a thumbs up and subscribe to my channel. I'll see you in the next video. Till then, bye.\", 'metadata': {'url': 'https://www.youtube.com/watch?v=3NgmslFf33I'}, 'type': 'youtube'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = extractor.extract_content('https://www.youtube.com/watch?v=3NgmslFf33I')\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = extractor.extract_content('https://www.youtube.com/watch?v=3NgmslFf33I')\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
